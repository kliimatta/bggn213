---
title: "Class 7:  Machine Learning 1"
author: "Kalle Liimatta A59002114"
date:  2022-10-19
format: gfm
editor: visual
toc: True
---

# K-means clustering

K --\> the number of clusters you get. You have to tell it how many clusters you want.

Let's make up some data to cluster.

```{r}
#rnorm --> will pick numbers from a normal distribution
#you can adjust where the mean falls by adding a number in the second position
tmp <- c(rnorm(30, -3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

```{r}
#rev function reverses your vector
rev(c("a", "b", "C"))

```

The function to do k-means clustering in base R is called 'kmeans()'.

```{r}
#for kmeans(), need to worry about x and centers (see the help page)
#x is the data you're clustering
#centers is the number of clusters you want
km <- kmeans(x, centers=4, nstart=20)
km
```

What "component" of your result object details:

-   cluster size? --\> "size"

```{r}
#size will tell you how many points in each cluster
km$size
```

-   cluster assignment/membership? --\> "cluster"

```{r}
km$cluster
```

Q. Plot x colored by the kmeans cluster assignment and add cluster centers as blue points?

```{r}
plot(x, col=c(rep(1,30), rep(2,30)))
```

```{r}
plot(x, col=km$cluster)
```

```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=2)
```

```{r}
km$centers
```

#Hierarchical Clustering

The 'hclust()' function performs hierarchical clustering. The big advantage here is that I don't need to tell it "k", the number of clusters...

To run 'hclust()' I need to provide a distance matrix as input (not the original data).

For kmean(), it assumes Euclidean distance

```{r}
hc <- hclust( dist(x))
hc
```

```{r}
plot(hc)
#looking for where the biggest jump is
```

To get my 'main" result (cluster membership), I want to "cut" this tree to yield "branches" whose leaves are the members of the cluster.

```{r}
#h= the height you want to cut the tree at
cutree(hc, h=8)
```

More often we will use 'cutree()' with k=2 for example

```{r}
#can also do, give me cut that will give me # clusters
grps <-cutree(hc, k=2)
```

Make a plot of our 'hclust()' results i.e. our data colored by cluster assignment

```{r}
plot(x, col=grps)
```

# Principal component analysis (PCA)

Read data for UK food trends from online

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```

## **Q1. How many rows and columns?**

```{r}
nrow(x)
```

```{r}
ncol(x)
```

```{r}
dim(x)
```

```{r}
head(x)
```

We are getting 5 columns--\> we want the column with the foods to be our index

```{r}
# Note how the minus indexing works
rownames(x) <- x[,1] #makes the first column the row titles
head(x)
```

```{r}
#need to get rid of the "X" column which is now duplicated
x <- x[,-1]
head(x)
```

```{r}
dim(x)
```

Explore the data\-- basically plot, plot, and plot again

```{r}
#another way to set the correct row names
x<- read.csv(url, row.names=1)
head(x)
```

## **Q2. Which approach to solving the "row-names problem" do you prefer and why?**

I prefer the second approach using "row.names=1". This way you don't have to worry about your matrix changing if you accidentally run your code again.

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

## **Q3. Changing what optional argument in the above barplot() function results in the following plot?**

Setting beside to False

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

## **Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?**

If a point lies on the diagonal, then that food has the same frequency in both countries displayed.

```{r}
pairs(x, col=rainbow(10), pch=16)
```

## **Q6. What is the main difference between N. Ireland and the other countries of the UK in terms of this data-set?**

#PCA to the rescue!

The main function in base R to do PCA is called 'prcomp()'.

One annoying thing about prcomp() function is that it expects the transpose of our data as input. --\> in this case, it wants the foods to be columns, and the countries to be rows

```{r}
t(x)
```

```{r}
pca <- prcomp(t(x))
summary(pca)
#most important bit is the variance
```

## **Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points**

The object returned by 'prcomp()' has our results that include a \$x component. This is our "scores" along the PCs (i.e. the plot of our data along the new PC axis).

```{r}
pca$x
```

```{r}
plot(pca$x[,1], pca$x[,2])
```

```{r}
plot(pca$x[,1], pca$x[,2], 
     xlab="PC1", ylab="PC2")
```

```{r}
plot(pca$x[,1], pca$x[,2], 
     xlab="PC1", ylab="PC2",
     col=c("orange", "red", "blue", "darkgreen"),
           pch=16)
```

```{r}
plot(pca$x[,1], pca$x[,2], 
     xlab="PC1", ylab="PC2",
     col=c("orange", "red", "blue", "darkgreen"),
           pch=16)
text(pca$x[,1], pca$x[,2], colnames(x))
```

## **Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.**

```{r}
plot(pca$x[,1], pca$x[,2], 
     xlab="PC1", ylab="PC2",
     col=c("orange", "red", "blue", "darkgreen"),
           pch=16)
text(pca$x[,1], pca$x[,2], colnames(x), col=c("orange", "red", "blue", "darkgreen"))
```

## Digging Deeper: Variable Loadings

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```
